.version 8.0
.target sm_80
.visible .entry matrix_transpose_kernel(
	.param .u64 param9,
	.param .u64 param10,
	.param .u64 param11,
	.param .u64 param12
) {
	.reg .u32 %r0, %r1, %r2, %r3, %r4, %r7, %r8, %r13, %r14, %r15, %r20, %r23, %r24, %r29, %r32, %r33, %r36;
	.reg .u64 %rd5, %rd9, %rd10, %rd11, %rd12, %rd16, %rd18, %rd19, %rd21, %rd22, %rd25, %rd26, %rd27, %rd28, %rd30, %rd31, %rd34, %rd35;
	.reg .pred %p6, %p17;
matrix_transpose_kernel_start:
	ld.param.u64 %rd9, [param9];
	ld.param.u64 %rd10, [param10];
	ld.param.u64 %rd11, [param11];
	ld.param.u64 %rd12, [param12];
block_13_start:
	mov.u32 %r0, %ntid.y;
	mov.u32 %r1, %ctaid.y;
	mul.lo.s32 %r2, %r0, %r1;
	mov.u32 %r3, %tid.y;
	add.u32 %r4, %r2, %r3;
	mov.u32 %r1, %r4;
	cvt.u64.u32 %rd5, %r4;
	setp.ge.u64 %p6, %rd5, %rd11;
	@%p6 bra block_13_end;
	mov.u32 %r7, %ntid.x;
	mov.u32 %r8, %ctaid.x;
	mul.lo.s32 %r13, %r7, %r8;
	mov.u32 %r14, %tid.x;
	add.u32 %r15, %r13, %r14;
	mov.u32 %r0, %r15;
	cvt.u64.u32 %rd16, %r15;
	setp.ge.u64 %p17, %rd16, %rd12;
	@%p17 bra block_13_end;
	cvt.u64.u32 %rd18, %r0;
	mul.lo.s64 %rd19, %rd18, %rd11;
	mov.u32 %r20, 2;
	shl.b64 %rd21, %rd19, %r20;
	add.u64 %rd22, %rd10, %rd21;
	mov.u32 %r23, 2;
	shl.b32 %r24, %r1, %r23;
	cvt.u64.u32 %rd25, %r24;
	add.u64 %rd26, %rd22, %rd25;
	cvt.u64.u32 %rd27, %r1;
	mul.lo.s64 %rd28, %rd27, %rd12;
	mov.u32 %r29, 2;
	shl.b64 %rd30, %rd28, %r29;
	add.u64 %rd31, %rd9, %rd30;
	mov.u32 %r32, 2;
	shl.b32 %r33, %r0, %r32;
	cvt.u64.u32 %rd34, %r33;
	add.u64 %rd35, %rd31, %rd34;
	ld.global.u32 %r36, [%rd35];
	st.global.u32 [%rd26], %r36;
block_13_end:
matrix_transpose_kernel_end:
}

