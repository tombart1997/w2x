.version 8.0
.target sm_80
.visible .entry matrix_mul_kernel(
	.param .u64 param9,
	.param .u64 param10,
	.param .u64 param11,
	.param .u64 param12,
	.param .u64 param13,
	.param .u64 param14
) {
	.reg .u32 %r0, %r1, %r2, %r3, %r4, %r7, %r8, %r15, %r16, %r17, %r20, %r23, %r24, %r31, %r35, %r36, %r38, %r41, %r43, %r45, %r51, %r54, %r55, %r56, %r57, %r59, %r67, %r70;
	.reg .u64 %rd5, %rd9, %rd10, %rd11, %rd12, %rd13, %rd14, %rd18, %rd21, %rd22, %rd25, %rd26, %rd27, %rd28, %rd29, %rd30, %rd32, %rd33, %rd34, %rd39, %rd46, %rd47, %rd48, %rd49, %rd50, %rd52, %rd53, %rd58, %rd60, %rd61, %rd62, %rd63, %rd64, %rd65, %rd66, %rd68, %rd71, %rd72;
	.reg .pred %p6, %p19, %p37, %p40, %p42, %p44, %p69;
matrix_mul_kernel_start:
	ld.param.u64 %rd9, [param9];
	ld.param.u64 %rd10, [param10];
	ld.param.u64 %rd11, [param11];
	ld.param.u64 %rd12, [param12];
	ld.param.u64 %rd13, [param13];
	ld.param.u64 %rd14, [param14];
block_5_start:
	mov.u32 %r0, %ctaid.y;
	mov.u32 %r1, %ntid.y;
	mul.lo.s32 %r2, %r0, %r1;
	mov.u32 %r3, %tid.y;
	add.u32 %r4, %r2, %r3;
	mov.u32 %r1, %r4;
	cvt.u64.u32 %rd5, %r4;
	setp.ge.u64 %p6, %rd5, %rd12;
	@%p6 bra block_5_end;
	mov.u32 %r7, %ctaid.x;
	mov.u32 %r8, %ntid.x;
	mul.lo.s32 %r15, %r7, %r8;
	mov.u32 %r16, %tid.x;
	add.u32 %r17, %r15, %r16;
	mov.u32 %r16, %r17;
	cvt.u64.u32 %rd18, %r17;
	setp.ge.u64 %p19, %rd18, %rd14;
	@%p19 bra block_5_end;
	mov.u32 %r20, 2;
	shl.b64 %rd21, %rd14, %r20;
	mov.u64 %rd22, %rd21;
	mov.u32 %r23, 2;
	shl.b32 %r24, %r16, %r23;
	cvt.u64.u32 %rd25, %r24;
	add.u64 %rd26, %rd10, %rd25;
	mov.u64 %rd27, %rd26;
	cvt.u64.u32 %rd28, %r1;
	mul.lo.s64 %rd29, %rd28, %rd13;
	mov.u64 %rd30, %rd29;
	mov.u32 %r31, 2;
	shl.b64 %rd32, %rd29, %r31;
	add.u64 %rd33, %rd9, %rd32;
	mov.u64 %rd34, %rd33;
	mov.u32 %r35, 0;
	mov.u32 %r36, %r35;
	mov.u32 %r3, %r16;
block_6_start:
loop_0_0_start:
	setp.ne.u64 %p37, %rd13, 0;
	@!%p37 bra if_3_end;
if_3_start:
	mov.u32 %r38, 1073741823;
	cvt.u64.u32 %rd39, %r38;
	setp.gt.u64 %p40, %rd30, %rd39;
	mov.u32 %r41, 1073741823;
	setp.gt.u32 %p42, %r3, %r41;
	or.b32 %r43, %p40, %p42;
	setp.ne.u32 %p44, %r43, 0;
	@%p44 bra block_6_end;
	mov.u32 %r45, 1;
	cvt.u64.u32 %rd46, %r45;
	sub.u64 %rd47, %rd13, %rd46;
	mov.u64 %rd13, %rd47;
	cvt.u64.u32 %rd48, %r3;
	add.u64 %rd49, %rd48, %rd14;
	mov.u64 %rd50, %rd49;
	mov.u32 %r51, 1;
	cvt.u64.u32 %rd52, %r51;
	add.u64 %rd53, %rd30, %rd52;
	mov.u64 %rd30, %rd53;
	ld.global.u32 %r54, [%rd27];
	ld.global.u32 %r55, [%rd34];
	mul.lo.s32 %r56, %r54, %r55;
	add.u32 %r57, %r56, %r36;
	mov.u32 %r36, %r57;
	add.u64 %rd58, %rd27, %rd22;
	mov.u64 %rd27, %rd58;
	mov.u32 %r59, 4;
	cvt.u64.u32 %rd60, %r59;
	add.u64 %rd61, %rd34, %rd60;
	mov.u64 %rd34, %rd61;
	bra loop_0_0_start;
if_3_end:
loop_0_0_end:
	cvt.u64.u32 %rd62, %r1;
	mul.lo.s64 %rd63, %rd62, %rd14;
	cvt.u64.u32 %rd64, %r16;
	add.u64 %rd65, %rd63, %rd64;
	mov.u64 %rd66, %rd65;
	mov.u32 %r67, 1073741823;
	cvt.u64.u32 %rd68, %r67;
	setp.gt.u64 %p69, %rd65, %rd68;
	@%p69 bra block_6_end;
	mov.u32 %r70, 2;
	shl.b64 %rd71, %rd66, %r70;
	add.u64 %rd72, %rd11, %rd71;
	st.global.u32 [%rd72], %r36;
	bra block_5_end;
block_6_end:
	trap;
block_5_end:
matrix_mul_kernel_end:
}

