.version 8.0
.target sm_80
.visible .entry vector_add_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r6, %r21, %r8, %r4, %r9, %r12, %r7, %r13, %r5, %r18, %r22;
    .reg .u64 %rd3, %rd16, %rd17, %rd19, %rd14, %rd15, %rd20, %rd2, %rd1, %rd0;
    .reg .pred %p11, %p10;
    ld.param.u64 %rd0, [param9];
    ld.param.u64 %rd1, [param10];
    ld.param.u64 %rd2, [param11];
    ld.param.u64 %rd3, [param12];
    block_1_start:
    mov.u32 %r4, %ntid.x;
    mov.u32 %r5, %ctaid.x;
    mul.lo.s32 %r6, %r4, %r5;
    mov.u32 %r7, %tid.x;
    add.s32 %r8, %r6, %r7;
    mov.u32 %r5, %r8;
    cvt.u32.u64 %r9, %rd3;
    setp.ge.u32 %p10, %r8, %r9;
    not.pred %p11, %p10;
    @%p11 bra br_if_label_0;
    br_if_label_0:
    mov.u32 %r12, 2;
    shl.b32 %r13, %r5, %r12;
    mov.u32 %r5, %r13;
    cvt.u64.u32 %rd14, %r13;
    add.s64 %rd15, %rd2, %rd14;
    cvt.u64.u32 %rd16, %r5;
    add.s64 %rd17, %rd1, %rd16;
    ld.global.u32 %r18, [%rd17];
    cvt.u64.u32 %rd19, %r5;
    add.s64 %rd20, %rd0, %rd19;
    ld.global.u32 %r21, [%rd20];
    add.s32 %r22, %r18, %r21;
    st.global.u32 [%rd15], %r22;
}

.visible .entry vector_add_loop_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r12, %r17, %r6, %r7, %r5, %r15, %r4, %r30, %r18, %r16, %r10, %r9, %r13, %r8, %r14, %r28, %r31, %r32, %r29, %r25;
    .reg .u64 %rd2, %rd23, %rd22, %rd27, %rd26, %rd1, %rd3, %rd0, %rd24, %rd21;
    .reg .pred %p19, %p20;
    ld.param.u64 %rd0, [param9];
    ld.param.u64 %rd1, [param10];
    ld.param.u64 %rd2, [param11];
    ld.param.u64 %rd3, [param12];
    mov.u32 %r4, %ntid.x;
    mov.u32 %r5, %ntid.z;
    mul.lo.s32 %r6, %r4, %r5;
    mov.u32 %r7, %ntid.y;
    mul.lo.s32 %r8, %r6, %r7;
    mov.u32 %r7, %r8;
    mov.u32 %r9, 2;
    shl.b32 %r10, %r8, %r9;
    mov.u32 %r11, %r10;
    mov.u32 %r12, %ctaid.x;
    mul.lo.s32 %r13, %r4, %r12;
    mov.u32 %r14, %tid.x;
    add.s32 %r15, %r13, %r14;
    mov.u32 %r5, %r15;
    mov.u32 %r16, 2;
    shl.b32 %r17, %r15, %r16;
    mov.u32 %r4, %r17;
    loop_0_start:
    bra loop_0_start;
    loop_0_end:
    block_2_start:
    cvt.u32.u64 %r18, %rd3;
    setp.ne.u64 %p19, 0, %rd3;
    setp.eq.u32 %p20, 0, %p19;
    @%p20 bra br_if_label_0;
    br_if_label_0:
    mov.u64 return_value_2, %rd2;
    mov.u64 return_value_1, %rd1;
    mov.u64 return_value_0, %rd0;
    ret;
    cvt.u64.u32 %rd21, %r4;
    add.s64 %rd22, %rd2, %rd21;
    cvt.u64.u32 %rd23, %r4;
    add.s64 %rd24, %rd1, %rd23;
    ld.global.u32 %r25, [%rd24];
    cvt.u64.u32 %rd26, %r4;
    add.s64 %rd27, %rd0, %rd26;
    ld.global.u32 %r28, [%rd27];
    add.s32 %r29, %r25, %r28;
    st.global.u32 [%rd22], %r29;
    add.s32 %r31, %r4, %r30;
    mov.u32 %r4, %r31;
    add.s32 %r32, %r5, %r7;
    mov.u32 %r5, %r32;
    bra br_if_label_0;
}

