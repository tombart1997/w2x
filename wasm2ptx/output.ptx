.version 8.0
.target sm_80
.visible .entry vector_add_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r1, %r17, %r8, %r0, %r5, %r20, %r21, %r3, %r4, %r2, %r7;
    .reg .u64 %rd19, %rd12, %rd15, %rd13, %rd14, %rd18, %rd10, %rd16, %rd9, %rd11;
    .reg .pred %p6;
    vector_add_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    block_1_start:
    mov.u32 %r0, %ntid.x;
    mov.u32 %r1, %ctaid.x;
    mul.lo.s32 %r2, %r0, %r1;
    mov.u32 %r3, %tid.x;
    add.s32 %r4, %r2, %r3;
    mov.u32 %r1, %r4;
    cvt.u32.u64 %r5, %rd12;
    setp.ge.u32 %p6, %r4, %r5;
    @%p6 bra block_1_end;
    mov.u32 %r7, 2;
    shl.b32 %r8, %r1, %r7;
    mov.u32 %r1, %r8;
    cvt.u64.u32 %rd13, %r8;
    add.s64 %rd14, %rd11, %rd13;
    cvt.u64.u32 %rd15, %r1;
    add.s64 %rd16, %rd10, %rd15;
    ld.global.u32 %r17, [%rd16];
    cvt.u64.u32 %rd18, %r1;
    add.s64 %rd19, %rd9, %rd18;
    ld.global.u32 %r20, [%rd19];
    add.s32 %r21, %r17, %r20;
    st.global.u32 [%rd14], %r21;
    block_1_end:
    vector_add_kernel_end:
}

.visible .entry vector_add_loop_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r18, %r30, %r17, %r5, %r2, %r7, %r3, %r15, %r6, %r13, %r29, %r0, %r1, %r27, %r8, %r24, %r4, %r28, %r16, %r14;
    .reg .u64 %rd22, %rd10, %rd26, %rd12, %rd20, %rd11, %rd23, %rd9, %rd25, %rd21;
    .reg .pred %p19;
    vector_add_loop_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    mov.u32 %r0, %ntid.x;
    mov.u32 %r1, %ntid.z;
    mul.lo.s32 %r2, %r0, %r1;
    mov.u32 %r3, %ntid.y;
    mul.lo.s32 %r4, %r2, %r3;
    mov.u32 %r3, %r4;
    mov.u32 %r5, 2;
    shl.b32 %r6, %r4, %r5;
    mov.u32 %r7, %r6;
    mov.u32 %r8, %ctaid.x;
    mul.lo.s32 %r13, %r0, %r8;
    mov.u32 %r14, %tid.x;
    add.s32 %r15, %r13, %r14;
    mov.u32 %r1, %r15;
    mov.u32 %r16, 2;
    shl.b32 %r17, %r15, %r16;
    mov.u32 %r0, %r17;
    loop_0_start:
    block_2_start:
    cvt.u32.u64 %r18, %rd12;
    setp.lt.u32 %p19, %r1, %r18;
    @%p19 bra block_2_end;
    ret;
    block_2_end:
    cvt.u64.u32 %rd20, %r0;
    add.s64 %rd21, %rd11, %rd20;
    cvt.u64.u32 %rd22, %r0;
    add.s64 %rd23, %rd10, %rd22;
    ld.global.u32 %r24, [%rd23];
    cvt.u64.u32 %rd25, %r0;
    add.s64 %rd26, %rd9, %rd25;
    ld.global.u32 %r27, [%rd26];
    add.s32 %r28, %r24, %r27;
    st.global.u32 [%rd21], %r28;
    add.s32 %r29, %r0, %r7;
    mov.u32 %r0, %r29;
    add.s32 %r30, %r1, %r3;
    mov.u32 %r1, %r30;
    bra loop_0_start;
    loop_0_end:
    vector_add_loop_kernel_end:
}

.visible .entry matrix_mul_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12,
    .param .u64 param13,
    .param .u64 param14
) {
    .reg .u32 %r53, %r8, %r3, %r19, %r29, %r55, %r36, %r22, %r37, %r15, %r26, %r32, %r58, %r59, %r33, %r46, %r45, %r25, %r0, %r30, %r21, %r31, %r49, %r56, %r16, %r4, %r18, %r62, %r41, %r42, %r40, %r2, %r23, %r17, %r57, %r39, %r6, %r1, %r5;
    .reg .u64 %rd54, %rd44, %rd14, %rd61, %rd48, %rd51, %rd52, %rd34, %rd35, %rd28, %rd12, %rd43, %rd47, %rd9, %rd13, %rd24, %rd27, %rd11, %rd10, %rd60, %rd50;
    .reg .pred %p20, %p38, %p7;
    matrix_mul_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    ld.param.u64 %rd13, [param13];
    ld.param.u64 %rd14, [param14];
    block_3_start:
    mov.u32 %r0, %ntid.y;
    mov.u32 %r1, %ctaid.y;
    mul.lo.s32 %r2, %r0, %r1;
    mov.u32 %r3, %tid.y;
    add.s32 %r4, %r2, %r3;
    mov.u32 %r5, %r4;
    cvt.u32.u64 %r6, %rd12;
    setp.ge.u32 %p7, %r4, %r6;
    @%p7 bra block_3_end;
    mov.u32 %r8, %ntid.x;
    mov.u32 %r15, %ctaid.x;
    mul.lo.s32 %r16, %r8, %r15;
    mov.u32 %r17, %tid.x;
    add.s32 %r18, %r16, %r17;
    mov.u32 %r15, %r18;
    cvt.u32.u64 %r19, %rd14;
    setp.ge.u32 %p20, %r18, %r19;
    @%p20 bra block_3_end;
    mov.u32 %r21, 2;
    cvt.u32.u64 %r22, %rd14;
    shl.b32 %r23, %r22, %r21;
    cvt.u64.u32 %rd24, %r23;
    mov.u64 %rd12, %rd24;
    mov.u32 %r25, 2;
    shl.b32 %r26, %r15, %r25;
    cvt.u64.u32 %rd27, %r26;
    add.s64 %rd28, %rd10, %rd27;
    cvt.u32.u64 %r29, %rd28;
    mov.u32 %r1, %r29;
    cvt.u32.u64 %r30, %rd13;
    mul.lo.s32 %r31, %r5, %r30;
    mov.u32 %r32, 2;
    shl.b32 %r33, %r31, %r32;
    cvt.u64.u32 %rd34, %r33;
    add.s64 %rd35, %rd9, %rd34;
    cvt.u32.u64 %r36, %rd35;
    mov.u32 %r0, %r36;
    mov.u32 %r37, 0;
    mov.u32 %r3, %r37;
    loop_0_start:
    block_4_start:
    setp.ne.u64 %p38, %rd13, 0;
    @%p38 bra block_4_end;
    cvt.u32.u64 %r39, %rd14;
    mul.lo.s32 %r40, %r5, %r39;
    mov.u32 %r41, 2;
    shl.b32 %r42, %r40, %r41;
    cvt.u64.u32 %rd43, %r42;
    add.s64 %rd44, %rd11, %rd43;
    mov.u32 %r45, 2;
    shl.b32 %r46, %r15, %r45;
    cvt.u64.u32 %rd47, %r46;
    add.s64 %rd48, %rd44, %rd47;
    st.global.u32 [%rd48], %r3;
    bra block_3_end;
    block_4_end:
    mov.s32 %r49, -1;
    cvt.u64.u32 %rd50, %r49;
    add.s64 %rd51, %rd13, %rd50;
    mov.u64 %rd13, %rd51;
    cvt.u64.u32 %rd52, %r1;
    ld.global.u32 %r53, [%rd52];
    cvt.u64.u32 %rd54, %r0;
    ld.global.u32 %r55, [%rd54];
    mul.lo.s32 %r56, %r53, %r55;
    add.s32 %r57, %r56, %r3;
    mov.u32 %r3, %r57;
    mov.u32 %r58, 4;
    add.s32 %r59, %r0, %r58;
    mov.u32 %r0, %r59;
    cvt.u64.u32 %rd60, %r1;
    add.s64 %rd61, %rd60, %rd12;
    cvt.u32.u64 %r62, %rd61;
    mov.u32 %r1, %r62;
    bra loop_0_start;
    loop_0_end:
    block_3_end:
    matrix_mul_kernel_end:
}

