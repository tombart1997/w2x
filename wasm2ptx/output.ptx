.version 8.0
.target sm_80
.visible .entry vector_add_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r16, %r22, %r0, %r2, %r3, %r5, %r14, %r1, %r4, %r23, %r8, %r15, %r26, %r19, %r27;
    .reg .u64 %rd11, %rd10, %rd9, %rd18, %rd6, %rd20, %rd24, %rd12, %rd25, %rd21, %rd17;
    .reg .pred %p7, %p13;
    vector_add_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    block_1_start:
    block_2_start:
    mov.u32 %r0, %ntid.x;
    mov.u32 %r1, %ctaid.x;
    mul.lo.s32 %r2, %r1, %r0;
    mov.u32 %r3, %tid.x;
    add.u32 %r4, %r2, %r3;
    mov.u32 %r5, %r4;
    cvt.u64.u32 %rd6, %r5;
    setp.gt.u64 %p7, %rd12, %rd6;
    if_1_start:
    mov.u32 %r8, 1073741823;
    setp.gt.u32 %p13, %r3, %r8;
    @%p13 bra block_2_end;
    mov.u32 %r14, 2;
    mov.u32 %r15, %ctaid.x;
    shl.b32 %r16, %r15, %r14;
    mov.u32 %r5, %r16;
    cvt.u64.u32 %rd17, %r5;
    add.u64 %rd18, %rd11, %rd17;
    mov.u32 %r19, %ctaid.x;
    cvt.u64.u32 %rd20, %r19;
    add.u64 %rd21, %rd20, %rd10;
    ld.global.u32 %r22, [%rd21];
    mov.u32 %r23, %ctaid.x;
    cvt.u64.u32 %rd24, %r23;
    add.u64 %rd25, %rd24, %rd9;
    ld.global.u32 %r26, [%rd25];
    add.u32 %r27, %r22, %r26;
    st.global.u32 [%rd18], %r27;
    if_1_end:
    bra block_1_end;
    block_2_end:
    trap;
    block_1_end:
    vector_add_kernel_end:
}

.visible .entry vector_add_loop_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r39, %r42, %r1, %r38, %r43, %r47, %r48, %r26, %r28, %r46, %r44, %r15, %r4, %r29, %r13, %r35, %r32, %r0, %r24, %r14, %r19, %r18, %r21, %r25, %r2, %r3, %r5, %r27, %r30, %r20, %r8, %r17, %r49, %r45, %r22, %r23, %r16;
    .reg .u64 %rd10, %rd41, %rd9, %rd33, %rd34, %rd36, %rd40, %rd6, %rd50, %rd12, %rd11, %rd37;
    .reg .pred %p7, %p31, %p51;
    vector_add_loop_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    block_3_start:
    block_4_start:
    mov.u32 %r0, %ntid.x;
    mov.u32 %r1, %ctaid.x;
    mul.lo.s32 %r2, %r1, %r0;
    mov.u32 %r3, %tid.x;
    add.u32 %r4, %r2, %r3;
    mov.u32 %r5, %r4;
    cvt.u64.u32 %rd6, %r5;
    setp.gt.u64 %p7, %rd12, %rd6;
    if_2_start:
    mov.u32 %r8, %ntid.y;
    mov.u32 %r13, %ntid.x;
    mul.lo.s32 %r14, %r13, %r8;
    mov.u32 %r15, %ntid.z;
    mul.lo.s32 %r16, %r14, %r15;
    mov.u32 %r17, %r16;
    mov.u32 %r18, 2;
    mov.u32 %r19, %tid.x;
    shl.b32 %r20, %r19, %r18;
    mov.u32 %r21, %r20;
    mov.u32 %r22, %ntid.z;
    mov.u32 %r23, %ntid.y;
    mul.lo.s32 %r24, %r23, %r22;
    mov.u32 %r25, %ntid.x;
    mul.lo.s32 %r26, %r24, %r25;
    mov.u32 %r27, 2;
    shl.b32 %r28, %r26, %r27;
    mov.u32 %r29, %r28;
    loop_0_start:
    mov.u32 %r30, 1073741823;
    setp.gt.u32 %p31, %r0, %r30;
    @%p31 bra block_4_end;
    mov.u32 %r32, %ctaid.x;
    cvt.u64.u32 %rd33, %r32;
    add.u64 %rd34, %rd33, %rd11;
    mov.u32 %r35, %ctaid.x;
    cvt.u64.u32 %rd36, %r35;
    add.u64 %rd37, %rd36, %rd10;
    ld.global.u32 %r38, [%rd37];
    mov.u32 %r39, %ctaid.x;
    cvt.u64.u32 %rd40, %r39;
    add.u64 %rd41, %rd40, %rd9;
    ld.global.u32 %r42, [%rd41];
    add.u32 %r43, %r38, %r42;
    st.global.u32 [%rd34], %r43;
    mov.u32 %r44, %ntid.x;
    mov.u32 %r45, %ctaid.x;
    add.u32 %r46, %r45, %r44;
    mov.u32 %r21, %r46;
    mov.u32 %r47, %tid.y;
    mov.u32 %r48, %tid.x;
    add.u32 %r49, %r48, %r47;
    mov.u32 %r5, %r49;
    cvt.u64.u32 %rd50, %r5;
    setp.lt.u64 %p51, %rd50, %rd12;
    @%p51 bra loop_0_start;
    loop_0_end:
    if_2_end:
    bra block_3_end;
    block_4_end:
    trap;
    block_3_end:
    vector_add_loop_kernel_end:
}

.visible .entry matrix_mul_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12,
    .param .u64 param13,
    .param .u64 param14
) {
    .reg .u32 %r57, %r72, %r42, %r4, %r46, %r41, %r70, %r71, %r91, %r54, %r16, %r24, %r69, %r26, %r90, %r37, %r15, %r66, %r74, %r78, %r18, %r17, %r2, %r67, %r31, %r50, %r63, %r48, %r58, %r53, %r22, %r3, %r60, %r32, %r73, %r19, %r61, %r86, %r89, %r68, %r59, %r81, %r85, %r33, %r94, %r62, %r5, %r0, %r8, %r1;
    .reg .u64 %rd13, %rd44, %rd10, %rd43, %rd39, %rd38, %rd55, %rd45, %rd47, %rd28, %rd30, %rd6, %rd36, %rd27, %rd40, %rd92, %rd9, %rd93, %rd25, %rd35, %rd79, %rd84, %rd12, %rd76, %rd52, %rd34, %rd11, %rd65, %rd14, %rd83, %rd29, %rd64, %rd80, %rd82, %rd87, %rd75, %rd51, %rd20;
    .reg .pred %p56, %p7, %p49, %p23, %p77, %p21, %p88;
    matrix_mul_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    ld.param.u64 %rd13, [param13];
    ld.param.u64 %rd14, [param14];
    block_5_start:
    mov.u32 %r0, %ntid.y;
    mov.u32 %r1, %ctaid.y;
    mul.lo.s32 %r2, %r1, %r0;
    mov.u32 %r3, %tid.y;
    add.u32 %r4, %r2, %r3;
    mov.u32 %r5, %r4;
    cvt.u64.u32 %rd6, %r5;
    setp.ge.u64 %p7, %rd6, %rd12;
    @%p7 bra block_5_end;
    mov.u32 %r8, %ntid.x;
    mov.u32 %r15, %ctaid.x;
    mul.lo.s32 %r16, %r15, %r8;
    mov.u32 %r17, %tid.x;
    add.u32 %r18, %r16, %r17;
    mov.u32 %r19, %r18;
    cvt.u64.u32 %rd20, %r19;
    setp.ge.u64 %p21, %rd20, %rd14;
    @%p21 bra block_5_end;
    block_6_start:
    block_7_start:
    cvt.u32.u64 %r22, %rd13;
    setp.eq.u32 %p23, %r22, 0;
    if_3_start:
    mov.u32 %r24, 0;
    cvt.u64.u32 %rd25, %r24;
    mov.u64 %rd12, %rd25;
    bra block_7_end;
    if_3_end:
    mov.u32 %r26, 2;
    shl.b64 %rd27, %rd14, %r26;
    mov.u64 %rd28, %rd27;
    sub.u64 %rd29, %tid.x, %rd14;
    mov.u64 %rd30, %rd29;
    mov.u32 %r31, 2;
    mov.u32 %r32, %tid.x;
    shl.b32 %r33, %r32, %r31;
    cvt.u64.u32 %rd34, %r33;
    add.u64 %rd35, %rd10, %rd34;
    mov.u64 %rd36, %rd35;
    mov.u32 %r37, %tid.z;
    cvt.u64.u32 %rd38, %r37;
    mul.lo.s64 %rd39, %rd38, %rd13;
    cvt.u32.u64 %r41, %rd39;
    mov.u64 %rd40, %r41;
    mov.u32 %r42, 2;
    shl.b64 %rd43, %rd40, %r42;
    add.u64 %rd44, %rd9, %rd43;
    mov.u64 %rd45, %rd44;
    mov.u32 %r46, 0;
    cvt.u64.u32 %rd47, %r46;
    mov.u64 %rd12, %rd47;
    loop_0_start:
    mov.u32 %r48, 1073741823;
    setp.gt.u32 %p49, %r4, %r48;
    @%p49 bra block_6_end;
    mov.u32 %r50, %ctaid.x;
    cvt.u64.u32 %rd51, %r50;
    add.u64 %rd52, %rd51, %rd14;
    cvt.u32.u64 %r53, %rd52;
    mov.u64 %rd30, %r53;
    mov.u32 %r54, 1073741823;
    cvt.u64.u32 %rd55, %r54;
    setp.gt.u64 %p56, %rd30, %rd55;
    @%p56 bra block_6_end;
    mov.u32 %r57, 1;
    mov.u32 %r58, %ctaid.y;
    add.u32 %r59, %r58, %r57;
    mov.u32 %r60, %r59;
    ld.global.u32 %r61, [%ntid.y];
    ld.global.u32 %r62, [%tid.y];
    mul.lo.s32 %r63, %r61, %r62;
    cvt.u64.u32 %rd64, %r63;
    add.u64 %rd65, %rd64, %rd12;
    mov.u64 %rd12, %rd65;
    mov.u32 %r66, 4;
    mov.u32 %r67, %tid.y;
    add.u32 %r68, %r67, %r66;
    mov.u32 %r69, %r68;
    mov.u32 %r70, %ntid.y;
    mov.u32 %r71, %ntid.x;
    add.u32 %r72, %r71, %r70;
    mov.u32 %r73, %r72;
    mov.u32 %r74, 1;
    cvt.u64.u32 %rd75, %r74;
    sub.u64 %rd76, %rd13, %rd75;
    setp.ne.u64 %p77, %rd76, 0;
    @%p77 bra loop_0_start;
    loop_0_end:
    block_7_end:
    mov.u32 %r78, %tid.z;
    cvt.u64.u32 %rd79, %r78;
    mul.lo.s64 %rd80, %rd79, %rd14;
    mov.u32 %r81, %tid.x;
    cvt.u64.u32 %rd82, %r81;
    add.u64 %rd83, %rd80, %rd82;
    cvt.u32.u64 %r85, %rd83;
    mov.u64 %rd84, %r85;
    mov.u32 %r86, 1073741823;
    cvt.u64.u32 %rd87, %r86;
    setp.gt.u64 %p88, %rd84, %rd87;
    @%p88 bra block_6_end;
    mov.u32 %r89, 2;
    mov.u32 %r90, %ctaid.y;
    shl.b32 %r91, %r90, %r89;
    cvt.u64.u32 %rd92, %r91;
    add.u64 %rd93, %rd11, %rd92;
    cvt.u32.u64 %r94, %rd12;
    st.global.u32 [%rd93], %r94;
    bra block_5_end;
    block_6_end:
    trap;
    block_5_end:
    matrix_mul_kernel_end:
}

