.version 8.0
.target sm_80
.visible .entry vector_add_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r3, %r6, %r1, %r2, %r20, %r8, %r17, %r21, %r13, %r4, %r14, %r0;
    .reg .u64 %rd15, %rd19, %rd11, %rd16, %rd10, %rd12, %rd18, %rd22, %rd9;
    .reg .pred %p5, %p7;
    vector_add_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    block_1_start:
    block_2_start:
    mov.u32 %r0, %ctaid.x;
    mov.u32 %r1, %ntid.x;
    mul.lo.s32 %r2, %r0, %r1;
    mov.u32 %r3, %tid.x;
    add.u32 %r4, %r2, %r3;
    mov.u32 %r0, %ctaid.x;
    setp.gt.u32 %p5, %r4, %r0;
    if_1_start:
    mov.u32 %r0, %ctaid.x;
    mov.u32 %r6, 1073741823;
    setp.gt.u32 %p7, %r0, %r6;
    @%p7 bra block_2_end;
    mov.u32 %r0, %ctaid.x;
    mov.u32 %r8, 2;
    shl.b32 %r13, %r0, %r8;
    mov.u32 %r0, %ctaid.x;
    add.u32 %r14, %r13, %r0;
    mov.u32 %r0, %ctaid.x;
    cvt.u64.u32 %rd15, %r0;
    add.u64 %rd16, %rd15, %rd10;
    ld.global.u32 %r17, [%rd16];
    mov.u32 %r0, %ctaid.x;
    cvt.u64.u32 %rd18, %r0;
    add.u64 %rd19, %rd18, %rd9;
    ld.global.u32 %r20, [%rd19];
    add.u32 %r21, %r17, %r20;
    cvt.u64.u32 %rd22, %r14;
    st.global.u32 [%rd22], %r21;
    if_1_end:
    bra block_1_end;
    block_2_end:
    trap;
    block_1_end:
    vector_add_kernel_end:
}

.visible .entry vector_add_loop_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12
) {
    .reg .u32 %r3, %r30, %r31, %r0, %r19, %r21, %r6, %r16, %r13, %r4, %r8, %r14, %r18, %r20, %r33, %r2, %r7, %r32, %r27, %r1, %r15, %r17;
    .reg .u64 %rd28, %rd34, %rd9, %rd11, %rd12, %rd25, %rd26, %rd23, %rd24, %rd10, %rd29;
    .reg .pred %p35, %p22, %p5;
    vector_add_loop_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    block_3_start:
    block_4_start:
    mov.u32 %r0, %ctaid.x;
    mov.u32 %r1, %ntid.x;
    mul.lo.s32 %r2, %r0, %r1;
    mov.u32 %r3, %tid.x;
    add.u32 %r4, %r2, %r3;
    mov.u32 %r3, %tid.x;
    setp.gt.u32 %p5, %r4, %r3;
    if_2_start:
    mov.u32 %r1, %ntid.x;
    mov.u32 %r6, %ntid.y;
    mul.lo.s32 %r7, %r1, %r6;
    mov.u32 %r8, %ntid.z;
    mul.lo.s32 %r13, %r7, %r8;
    mov.u32 %r14, %r13;
    mov.u32 %r3, %tid.x;
    mov.u32 %r15, 2;
    shl.b32 %r16, %r3, %r15;
    mov.u32 %r0, %r16;
    mov.u32 %r6, %ntid.y;
    mov.u32 %r8, %ntid.z;
    mul.lo.s32 %r17, %r6, %r8;
    mov.u32 %r1, %ntid.x;
    mul.lo.s32 %r18, %r17, %r1;
    mov.u32 %r19, 2;
    shl.b32 %r20, %r18, %r19;
    mov.u32 %r1, %r20;
    loop_0_start:
    mov.u32 %r3, %tid.x;
    mov.u32 %r21, 1073741823;
    setp.gt.u32 %p22, %r3, %r21;
    @%p22 bra block_4_end;
    mov.u32 %r0, %ctaid.x;
    cvt.u64.u32 %rd23, %r0;
    add.u64 %rd24, %rd23, %rd11;
    mov.u32 %r0, %ctaid.x;
    cvt.u64.u32 %rd25, %r0;
    add.u64 %rd26, %rd25, %rd10;
    ld.global.u32 %r27, [%rd26];
    mov.u32 %r0, %ctaid.x;
    cvt.u64.u32 %rd28, %r0;
    add.u64 %rd29, %rd28, %rd9;
    ld.global.u32 %r30, [%rd29];
    add.u32 %r31, %r27, %r30;
    st.global.u32 [%rd24], %r31;
    mov.u32 %r0, %ctaid.x;
    mov.u32 %r1, %ntid.x;
    add.u32 %r32, %r0, %r1;
    mov.u32 %r0, %r32;
    mov.u32 %r3, %tid.x;
    mov.u32 %r14, %tid.y;
    add.u32 %r33, %r3, %r14;
    mov.u32 %r3, %tid.x;
    cvt.u64.u32 %rd34, %r3;
    setp.lt.u64 %p35, %rd34, %rd12;
    @%p35 bra loop_0_start;
    loop_0_end:
    if_2_end:
    bra block_3_end;
    block_4_end:
    trap;
    block_3_end:
    vector_add_loop_kernel_end:
}

.visible .entry matrix_mul_kernel(
    .param .u64 param9,
    .param .u64 param10,
    .param .u64 param11,
    .param .u64 param12,
    .param .u64 param13,
    .param .u64 param14
) {
    .reg .u32 %r45, %r4, %r1, %r16, %r75, %r65, %r60, %r17, %r47, %r63, %r77, %r66, %r81, %r50, %r21, %r59, %r38, %r52, %r53, %r2, %r23, %r5, %r8, %r31, %r43, %r57, %r67, %r18, %r15, %r56, %r64, %r39, %r54, %r78, %r25, %r0, %r32, %r3;
    .reg .u64 %rd74, %rd55, %rd34, %rd13, %rd68, %rd73, %rd41, %rd36, %rd26, %rd27, %rd29, %rd79, %rd72, %rd37, %rd40, %rd42, %rd12, %rd35, %rd11, %rd19, %rd69, %rd80, %rd24, %rd14, %rd10, %rd44, %rd9, %rd33, %rd61, %rd71, %rd62, %rd30, %rd49, %rd48, %rd58, %rd28, %rd6;
    .reg .pred %p22, %p70, %p51, %p46, %p20, %p76, %p7;
    matrix_mul_kernel_start:
    ld.param.u64 %rd9, [param9];
    ld.param.u64 %rd10, [param10];
    ld.param.u64 %rd11, [param11];
    ld.param.u64 %rd12, [param12];
    ld.param.u64 %rd13, [param13];
    ld.param.u64 %rd14, [param14];
    block_5_start:
    mov.u32 %r0, %ctaid.y;
    mov.u32 %r1, %ntid.y;
    mul.lo.s32 %r2, %r0, %r1;
    mov.u32 %r3, %tid.y;
    add.u32 %r4, %r2, %r3;
    mov.u32 %r5, %tid.z;
    cvt.u64.u32 %rd6, %r5;
    setp.ge.u64 %p7, %rd6, %rd12;
    @%p7 bra block_5_end;
    mov.u32 %r8, %ctaid.x;
    mov.u32 %r15, %ntid.x;
    mul.lo.s32 %r16, %r8, %r15;
    mov.u32 %r17, %tid.x;
    add.u32 %r18, %r16, %r17;
    mov.u32 %r17, %tid.x;
    cvt.u64.u32 %rd19, %r17;
    setp.ge.u64 %p20, %rd19, %rd14;
    @%p20 bra block_5_end;
    block_6_start:
    block_7_start:
    cvt.u32.u64 %r21, %rd13;
    setp.eq.u32 %p22, %r21, 0;
    if_3_start:
    mov.u32 %r23, 0;
    cvt.u64.u32 %rd24, %r23;
    mov.u64 %rd12, %rd24;
    bra block_7_end;
    if_3_end:
    mov.u32 %r25, 2;
    shl.b64 %rd26, %rd14, %r25;
    mov.u64 %rd27, %rd26;
    mov.u32 %r17, %tid.x;
    cvt.u64.u32 %rd28, %r17;
    sub.u64 %rd29, %rd28, %rd14;
    mov.u64 %rd30, %rd29;
    mov.u32 %r17, %tid.x;
    mov.u32 %r31, 2;
    shl.b32 %r32, %r17, %r31;
    cvt.u64.u32 %rd33, %r32;
    add.u64 %rd34, %rd10, %rd33;
    mov.u64 %rd35, %rd34;
    mov.u32 %r5, %tid.z;
    cvt.u64.u32 %rd36, %r5;
    mul.lo.s64 %rd37, %rd36, %rd13;
    mov.u32 %r0, %ctaid.y;
    mov.u32 %r38, 2;
    shl.b32 %r39, %r0, %r38;
    cvt.u64.u32 %rd40, %r39;
    add.u64 %rd41, %rd37, %rd40;
    mov.u64 %rd42, %rd41;
    mov.u32 %r43, 0;
    cvt.u64.u32 %rd44, %r43;
    mov.u64 %rd12, %rd44;
    loop_0_start:
    mov.u32 %r0, %ctaid.y;
    mov.u32 %r45, 1073741823;
    setp.gt.u32 %p46, %r0, %r45;
    @%p46 bra block_6_end;
    mov.u32 %r47, %ctaid.x;
    cvt.u64.u32 %rd48, %r47;
    add.u64 %rd49, %rd48, %rd14;
    mov.u32 %r47, %ctaid.x;
    mov.u32 %r50, 1073741823;
    setp.gt.u32 %p51, %r47, %r50;
    @%p51 bra block_6_end;
    mov.u32 %r0, %ctaid.y;
    mov.u32 %r52, 1;
    add.u32 %r53, %r0, %r52;
    mov.u32 %r0, %r53;
    mov.u32 %r54, %ntid.y;
    cvt.u64.u32 %rd55, %r54;
    ld.global.u32 %r56, [%rd55];
    mov.u32 %r57, %tid.y;
    cvt.u64.u32 %rd58, %r57;
    ld.global.u32 %r59, [%rd58];
    mul.lo.s32 %r60, %r56, %r59;
    cvt.u64.u32 %rd61, %r60;
    add.u64 %rd62, %rd61, %rd12;
    mov.u64 %rd12, %rd62;
    mov.u32 %r57, %tid.y;
    mov.u32 %r63, 4;
    add.u32 %r64, %r57, %r63;
    mov.u32 %r57, %r64;
    mov.u32 %r65, %ntid.x;
    mov.u32 %r54, %ntid.y;
    add.u32 %r66, %r65, %r54;
    mov.u32 %r54, %r66;
    mov.u32 %r67, 1;
    cvt.u64.u32 %rd68, %r67;
    sub.u64 %rd69, %rd13, %rd68;
    mov.u64 %rd13, %rd69;
    setp.ne.u64 %p70, %rd69, 0;
    @%p70 bra loop_0_start;
    loop_0_end:
    block_7_end:
    mov.u32 %r5, %tid.z;
    cvt.u64.u32 %rd71, %r5;
    mul.lo.s64 %rd72, %rd71, %rd14;
    mov.u32 %r17, %tid.x;
    cvt.u64.u32 %rd73, %r17;
    add.u64 %rd74, %rd72, %rd73;
    mov.u32 %r0, %ctaid.y;
    mov.u32 %r75, 1073741823;
    setp.gt.u32 %p76, %r0, %r75;
    @%p76 bra block_6_end;
    mov.u32 %r0, %ctaid.y;
    mov.u32 %r77, 2;
    shl.b32 %r78, %r0, %r77;
    cvt.u64.u32 %rd79, %r78;
    add.u64 %rd80, %rd11, %rd79;
    cvt.u32.u64 %r81, %rd12;
    st.global.u32 [%rd80], %r81;
    bra block_5_end;
    block_6_end:
    trap;
    block_5_end:
    matrix_mul_kernel_end:
}

